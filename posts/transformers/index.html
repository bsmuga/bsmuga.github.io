<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformers | Technical Basics</title>
<meta name="keywords" content="architecture, attention, transformer, foundation, long-read, reinforcement-learning" />
<meta name="description" content="This page covers the basics of Transformer architecture including notation and core concepts.
Notations

  
      
          Symbol
          Meaning
      
  
  
      
          $d$
          The model size / hidden state dimension / positional encoding size.
      
      
          $h$
          The number of heads in multi-head attention layer.
      
      
          $L$
          The segment length of input sequence.
      
      
          $N$
          The total number of attention layers in the model; not considering MoE.
      
      
          $\mathbf{X} \in \mathbb{R}^{L \times d}$
          The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.
      
      
          $\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$
          The key weight matrix.
      
      
          $\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$
          The query weight matrix.
      
      
          $\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$
          The value weight matrix. Often we have $d_k = d_v = d$.
      
      
          $\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$
          The weight matrices per head.
      
      
          $\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$
          The output weight matrix.
      
      
          $\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$
          The query embedding inputs.
      
      
          $\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$
          The key embedding inputs.
      
      
          $\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$
          The value embedding inputs.
      
      
          $\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$
          Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.
      
      
          $S_i$
          A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.
      
      
          $\mathbf{A} \in \mathbb{R}^{L \times L}$
          The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.
      
      
          $a_{ij} \in \mathbf{A}$
          The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.
      
      
          $\mathbf{P} \in \mathbb{R}^{L \times d}$
          position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.
      
  

Transformer Basics
The Transformer (which will be referred to as &ldquo;vanilla Transformer&rdquo; to distinguish it from other enhanced versions; Vaswani, et al., 2017) model has an encoder-decoder architecture, as commonly used in many NMT models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only BERT or decoder-only GPT.">
<meta name="author" content="Bartosz Smuga">
<link rel="canonical" href="/posts/2025-01-27-the-transformer-family-v2/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.51b2420ff5ea1215cdf584af7ba59d5fea94201c33f25109d6448c7271631316.css" integrity="sha256-UbJCD/XqEhXN9YSve6WdX&#43;qUIBwz8lEJ1kSMcnFjExY=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js" integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon_wine.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="/posts/2025-01-27-the-transformer-family-v2/" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-HFT45VFBX6');
        }
      </script><meta property="og:title" content="Transformers" />
<meta property="og:description" content="This page covers the basics of Transformer architecture including notation and core concepts.
Notations

  
      
          Symbol
          Meaning
      
  
  
      
          $d$
          The model size / hidden state dimension / positional encoding size.
      
      
          $h$
          The number of heads in multi-head attention layer.
      
      
          $L$
          The segment length of input sequence.
      
      
          $N$
          The total number of attention layers in the model; not considering MoE.
      
      
          $\mathbf{X} \in \mathbb{R}^{L \times d}$
          The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.
      
      
          $\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$
          The key weight matrix.
      
      
          $\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$
          The query weight matrix.
      
      
          $\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$
          The value weight matrix. Often we have $d_k = d_v = d$.
      
      
          $\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$
          The weight matrices per head.
      
      
          $\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$
          The output weight matrix.
      
      
          $\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$
          The query embedding inputs.
      
      
          $\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$
          The key embedding inputs.
      
      
          $\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$
          The value embedding inputs.
      
      
          $\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$
          Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.
      
      
          $S_i$
          A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.
      
      
          $\mathbf{A} \in \mathbb{R}^{L \times L}$
          The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.
      
      
          $a_{ij} \in \mathbf{A}$
          The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.
      
      
          $\mathbf{P} \in \mathbb{R}^{L \times d}$
          position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.
      
  

Transformer Basics
The Transformer (which will be referred to as &ldquo;vanilla Transformer&rdquo; to distinguish it from other enhanced versions; Vaswani, et al., 2017) model has an encoder-decoder architecture, as commonly used in many NMT models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only BERT or decoder-only GPT." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/2025-01-27-the-transformer-family-v2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-27T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2025-01-27T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Transformers"/>
<meta name="twitter:description" content="This page covers the basics of Transformer architecture including notation and core concepts.
Notations

  
      
          Symbol
          Meaning
      
  
  
      
          $d$
          The model size / hidden state dimension / positional encoding size.
      
      
          $h$
          The number of heads in multi-head attention layer.
      
      
          $L$
          The segment length of input sequence.
      
      
          $N$
          The total number of attention layers in the model; not considering MoE.
      
      
          $\mathbf{X} \in \mathbb{R}^{L \times d}$
          The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.
      
      
          $\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$
          The key weight matrix.
      
      
          $\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$
          The query weight matrix.
      
      
          $\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$
          The value weight matrix. Often we have $d_k = d_v = d$.
      
      
          $\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$
          The weight matrices per head.
      
      
          $\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$
          The output weight matrix.
      
      
          $\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$
          The query embedding inputs.
      
      
          $\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$
          The key embedding inputs.
      
      
          $\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$
          The value embedding inputs.
      
      
          $\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$
          Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.
      
      
          $S_i$
          A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.
      
      
          $\mathbf{A} \in \mathbb{R}^{L \times L}$
          The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.
      
      
          $a_{ij} \in \mathbf{A}$
          The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.
      
      
          $\mathbf{P} \in \mathbb{R}^{L \times d}$
          position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.
      
  

Transformer Basics
The Transformer (which will be referred to as &ldquo;vanilla Transformer&rdquo; to distinguish it from other enhanced versions; Vaswani, et al., 2017) model has an encoder-decoder architecture, as commonly used in many NMT models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only BERT or decoder-only GPT."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformers",
      "item": "/posts/2025-01-27-the-transformer-family-v2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Technical BlogPosting",
  "headline": "Transformers",
  "name": "Transformers",
  "description": "This page covers the basics of Transformer architecture including notation and core concepts.
  "keywords": [
    "architecture", "attention", "transformer", "foundation", "long-read", "reinforcement-learning"
  ],
  "articleBody": "This page covers the basics of Transformer architecture including notation and core concepts.
  "wordCount" : "9539",
  "inLanguage": "en",
  "datePublished": "2025-01-27T00:00:00Z",
  "dateModified": "2025-01-27T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Bartosz Smuga"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/posts/2025-01-27-the-transformer-family-v2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Technical Basics",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon_wine.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="Technical Basics (Alt + H)">Technical Basics</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Transformers
    </h1>
    <div class="post-meta">Date: December 13, 2025  |  Estimated Reading Time: 10 min  |  Author: Bartosz Smuga

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#notations" aria-label="Notations">Notations</a></li>
                <li>
                    <a href="#transformer-basics" aria-label="Transformer Basics">Transformer Basics</a><ul>
                        
                <li>
                    <a href="#attention-and-self-attention" aria-label="Attention and Self-Attention">Attention and Self-Attention</a></li>
                <li>
                    <a href="#multi-head-self-attention" aria-label="Multi-Head Self-Attention">Multi-Head Self-Attention</a></li>
                <li>
                    <a href="#encoder-decoder-architecture" aria-label="Encoder-Decoder Architecture">Encoder-Decoder Architecture</a></li>
                <li>
                    <a href="#positional-encoding" aria-label="Positional Encoding">Positional Encoding</a><ul>
                        
                <li>
                    <a href="#sinusoidal-positional-encoding" aria-label="Sinusoidal Positional Encoding">Sinusoidal Positional Encoding</a></li>
                <li>
                    <a href="#learned-positional-encoding" aria-label="Learned Positional Encoding">Learned Positional Encoding</a></li>
                <li>
                    <a href="#relative-position-encoding" aria-label="Relative Position Encoding">Relative Position Encoding</a></li>
                <li>
                    <a href="#rotary-position-embedding" aria-label="Rotary Position Embedding">Rotary Position Embedding</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>
  <div class="post-content"><p>This page covers the basics of Transformer architecture including notation and core concepts.</p>
<h1 id="notations">Notations<a hidden class="anchor" aria-hidden="true" href="#notations">#</a></h1>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$d$</td>
          <td>The model size / hidden state dimension / positional encoding size.</td>
      </tr>
      <tr>
          <td>$h$</td>
          <td>The number of heads in multi-head attention layer.</td>
      </tr>
      <tr>
          <td>$L$</td>
          <td>The segment length of input sequence.</td>
      </tr>
      <tr>
          <td>$N$</td>
          <td>The total number of attention layers in the model; not considering MoE.</td>
      </tr>
      <tr>
          <td>$\mathbf{X} \in \mathbb{R}^{L \times d}$</td>
          <td>The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.</td>
      </tr>
      <tr>
          <td>$\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$</td>
          <td>The key weight matrix.</td>
      </tr>
      <tr>
          <td>$\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$</td>
          <td>The query weight matrix.</td>
      </tr>
      <tr>
          <td>$\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$</td>
          <td>The value weight matrix. Often we have $d_k = d_v = d$.</td>
      </tr>
      <tr>
          <td>$\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$</td>
          <td>The weight matrices per head.</td>
      </tr>
      <tr>
          <td>$\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$</td>
          <td>The output weight matrix.</td>
      </tr>
      <tr>
          <td>$\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$</td>
          <td>The query embedding inputs.</td>
      </tr>
      <tr>
          <td>$\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$</td>
          <td>The key embedding inputs.</td>
      </tr>
      <tr>
          <td>$\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$</td>
          <td>The value embedding inputs.</td>
      </tr>
      <tr>
          <td>$\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$</td>
          <td>Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.</td>
      </tr>
      <tr>
          <td>$S_i$</td>
          <td>A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.</td>
      </tr>
      <tr>
          <td>$\mathbf{A} \in \mathbb{R}^{L \times L}$</td>
          <td>The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.</td>
      </tr>
      <tr>
          <td>$a_{ij} \in \mathbf{A}$</td>
          <td>The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.</td>
      </tr>
      <tr>
          <td>$\mathbf{P} \in \mathbb{R}^{L \times d}$</td>
          <td>position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.</td>
      </tr>
  </tbody>
</table>
<h1 id="transformer-basics">Transformer Basics<a hidden class="anchor" aria-hidden="true" href="#transformer-basics">#</a></h1>
<p>The <strong>Transformer</strong> (which will be referred to as &ldquo;vanilla Transformer&rdquo; to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model has an encoder-decoder architecture, as commonly used in many NMT models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only BERT or decoder-only GPT.</p>
<h2 id="attention-and-self-attention">Attention and Self-Attention<a hidden class="anchor" aria-hidden="true" href="#attention-and-self-attention">#</a></h2>
<p><strong>Attention</strong> is a mechanism in neural network that a model can learn to make predictions by selectively attending to a given set of data. The amount of attention is quantified by learned weights and thus the output is usually formed as a weighted average.</p>
<p><strong>Self-attention</strong> is a type of attention mechanism where the model makes prediction for one part of a data sample using other parts of the observation about the same sample. Conceptually, it feels quite similar to <a href="https://en.wikipedia.org/wiki/Non-local_means">non-local means</a>. Also note that self-attention is permutation-invariant; in other words, it is an operation on sets.</p>
<p>There are various forms of attention / self-attention, Transformer (<a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a>) relies on the <em>scaled dot-product attention</em>: given a query matrix $\mathbf{Q}$, a key matrix $\mathbf{K}$ and a value matrix $\mathbf{V}$, the output is a weighted sum of the value vectors, where the weight assigned to each value slot is determined by the dot-product of the query with the corresponding key:</p>
<div>
$$
\text{attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q} {\mathbf{K}}^\top}{\sqrt{d_k}})\mathbf{V}
$$
</div>
<p>And for a query and a key vector $\mathbf{q}_i, \mathbf{k}_j \in \mathbb{R}^d$ (row vectors in query and key matrices), we have a scalar score:</p>
<div>
$$
a_{ij} = \text{softmax}(\frac{\mathbf{q}_i {\mathbf{k}_j}^\top}{\sqrt{d_k}})
= \frac{\exp(\frac{\mathbf{q}_i {\mathbf{k}_j}^\top}{\sqrt{d_k}})}{ \sum_{r \in \mathcal{S}_i} \exp(\frac{\mathbf{q}_i {\mathbf{k}_r}^\top}{\sqrt{d_k}}) }
$$ 
</div>
<p>where $\mathcal{S}_i$ is a collection of key positions for the $i$-th query to attend to.</p>
<p>See my old <a href="/posts/2018-06-24-attention/#a-family-of-attention-mechanisms">post for other types of attention</a> if interested.</p>
<h2 id="multi-head-self-attention">Multi-Head Self-Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-self-attention">#</a></h2>
<p>The <strong>multi-head self-attention</strong> module is a key component in Transformer. Rather than only computing the attention once, the multi-head mechanism splits the inputs into smaller chunks and then computes the scaled dot-product attention over each subspace in parallel. The independent attention outputs are simply concatenated and linearly transformed into expected dimensions.</p>
<div>
$$
\begin{aligned}
\text{MultiHeadAttn}(\mathbf{X}_q, \mathbf{X}_k, \mathbf{X}_v) &= [\text{head}_1; \dots; \text{head}_h] \mathbf{W}^o \\ 
\text{where head}_i &= \text{Attention}(\mathbf{X}_q\mathbf{W}^q_i, \mathbf{X}_k\mathbf{W}^k_i, \mathbf{X}_v\mathbf{W}^v_i)
\end{aligned}
$$
</div>
<p>where $[.;.]$ is a concatenation operation. $\mathbf{W}^q_i, \mathbf{W}^k_i \in \mathbb{R}^{d \times d_k/h}, \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$ are weight matrices to map input embeddings of size $L \times d$ into query, key and value matrices. And $\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$ is the output linear transformation. All the weights should be learned during training.</p>
<figure>
	<img src="multi-head-attention.png" style="width: 35%;"  />
	<figcaption>Illustration of the multi-head scaled dot-product attention mechanism. (Image source: Figure 2 in <a href="https://arxiv.org/abs/1706.03762" target="_blank">Vaswani, et al., 2017</a>)</figcaption>
</figure>
<h2 id="encoder-decoder-architecture">Encoder-Decoder Architecture<a hidden class="anchor" aria-hidden="true" href="#encoder-decoder-architecture">#</a></h2>
<p>The <strong>encoder</strong> generates an attention-based representation with capability to locate a specific piece of information from a large context. It consists of a stack of 6 identity modules, each containing two submodules, a <em>multi-head self-attention</em> layer and a <em>point-wise</em> fully connected feed-forward network. By point-wise, it means that it applies the same linear transformation (with same weights) to each element in the sequence. This can also be viewed as a convolutional layer with filter size 1. Each submodule has a residual connection and layer normalization. All the submodules output data of the same dimension $d$.</p>
<p>The function of Transformer <strong>decoder</strong> is to retrieve information from the encoded representation. The architecture is quite similar to the encoder, except that the decoder contains two multi-head attention submodules instead of one in each identical repeating module. The first multi-head attention submodule is <em>masked</em> to prevent positions from attending to the future.</p>
<figure>
	<img src="transformer.png" style="width: 100%;"  />
	<figcaption>The architecture of the vanilla Transformer model. (Image source: <a href="/posts/2018-06-24-attention/#full-architecture" target="_blank">Figure 17</a>)</figcaption>
</figure>
<h2 id="positional-encoding">Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#positional-encoding">#</a></h2>
<p>Because self-attention operation is permutation invariant, it is important to use proper <strong>positional encoding</strong> to provide <em>order information</em> to the model. The positional encoding $\mathbf{P} \in \mathbb{R}^{L \times d}$ has the same dimension as the input embedding, so it can be added on the input directly. The vanilla Transformer considered two types of encodings:</p>
<h3 id="sinusoidal-positional-encoding">Sinusoidal Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#sinusoidal-positional-encoding">#</a></h3>
<p>Sinusoidal positional encoding is defined as follows, given the token position $i=1,\dots,L$ and the dimension $\delta=1,\dots,d$:</p>
<div>
$$
\text{PE}(i,\delta) = 
\begin{cases}
\sin(\frac{i}{10000^{2\delta'/d}}) & \text{if } \delta = 2\delta'\\
\cos(\frac{i}{10000^{2\delta'/d}}) & \text{if } \delta = 2\delta' + 1\\
\end{cases}
$$
</div>
<p>In this way each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions, from $2\pi$ to $10000 \cdot 2\pi$.</p>
<figure>
	<img src="sinoidual-positional-encoding.png" style="width: 100%;"  />
	<figcaption>Sinusoidal positional encoding with $L=32$ and $d=128$. The value is between -1 (black) and 1 (white) and the value 0 is in gray.</figcaption>
</figure>
<h3 id="learned-positional-encoding">Learned Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#learned-positional-encoding">#</a></h3>
<p>Learned positional encoding assigns each element with a <em>learned</em> column vector which encodes its absolute position (<a href="https://arxiv.org/abs/1705.03122">Gehring, et al. 2017</a>) and furthermroe this encoding can be learned differently per layer (<a href="https://arxiv.org/abs/1808.04444">Al-Rfou et al. 2018</a>).<br/><br/></p>
<h3 id="relative-position-encoding">Relative Position Encoding<a hidden class="anchor" aria-hidden="true" href="#relative-position-encoding">#</a></h3>
<p><a href="https://arxiv.org/abs/1803.02155">Shaw et al. (2018)</a>) incorporated relative positional information into $\mathbf{W}^k$ and $\mathbf{W}^v$. Maximum relative position is clipped to a maximum absolute value of $k$ and this clipping operation enables the model to generalize to unseen sequence lengths. Therefore, $2k + 1$ unique edge labels are considered and let us denote $\mathbf{P}^k, \mathbf{P}^v \in \mathbb{R}^{2k+1}$ as learnable relative position representations.</p>
<div>
$$
A_{ij}^k = P^k_{\text{clip}(j - i, k)} \quad
A_{ij}^v = P^v_{\text{clip}(j - i, k)} \quad
\text{where }\text{clip}(x, k) = \text{clip}(x, -k, k)
$$
</div><br/>
<p><a id="transformer-xl-encoding"></a><a href="#transformer-xl">Transformer-XL</a> (<a href="https://arxiv.org/abs/1901.02860">Dai et al., 2019</a>) proposed a type of relative positional encoding based on reparametrization of dot-product of keys and queries. To keep the positional information flow coherently across segments, Transformer-XL encodes the <em>relative</em> position instead, as it could be sufficient enough to know the position offset for making good predictions, i.e. $i-j$, between one key vector $\mathbf{k}_{\tau, j}$ and its query $\mathbf{q}_{\tau, i}$.</p>
<p>If omitting the scalar $1/\sqrt{d_k}$ and the normalizing term in softmax but including positional encodings, we can write the attention score between query at position $i$ and key at position $j$ as:</p>
<div>
$$
\begin{aligned}
a_{ij} 
&= \mathbf{q}_i {\mathbf{k}_j}^\top = (\mathbf{x}_i + \mathbf{p}_i)\mathbf{W}^q ((\mathbf{x}_j + \mathbf{p}_j)\mathbf{W}^k)^\top \\
&= \mathbf{x}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{x}_j^\top + \mathbf{x}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{p}_j^\top + \mathbf{p}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{x}_j^\top + \mathbf{p}_i\mathbf{W}^q {\mathbf{W}^k}^\top\mathbf{p}_j^\top
\end{aligned}
$$
</div>
<p>Transformer-XL reparameterizes the above four terms as follows:</p>
<div>
$$
a_{ij}^\text{rel} = 
\underbrace{ \mathbf{x}_i\mathbf{W}^q \color{blue}{ {\mathbf{W}_E^k}^\top } \mathbf{x}_j^\top }_\text{content-based addressing} + 
\underbrace{ \mathbf{x}_i\mathbf{W}^q \color{blue}{ {\mathbf{W}_R^k}^\top } \color{green}{\mathbf{r}_{i-j}^\top} }_\text{content-dependent positional bias} + 
\underbrace{ \color{red}{\mathbf{u}} \color{blue}{ {\mathbf{W}_E^k}^\top } \mathbf{x}_j^\top }_\text{global content bias} + 
\underbrace{ \color{red}{\mathbf{v}} \color{blue}{ {\mathbf{W}_R^k}^\top } \color{green}{\mathbf{r}_{i-j}^\top} }_\text{global positional bias}
$$
</div>
<ul>
<li>Replace $\mathbf{p}_j$ with relative positional encoding $\mathbf{r}_{i-j} \in \mathbf{R}^{d}$;</li>
<li>Replace $\mathbf{p}_i\mathbf{W}^q$ with two trainable parameters $\mathbf{u}$ (for content) and $\mathbf{v}$ (for location) in two different terms;</li>
<li>Split $\mathbf{W}^k$ into two matrices, $\mathbf{W}^k_E$ for content information and $\mathbf{W}^k_R$ for location information.</li>
</ul>
<h3 id="rotary-position-embedding">Rotary Position Embedding<a hidden class="anchor" aria-hidden="true" href="#rotary-position-embedding">#</a></h3>
<p>Rotary position embedding (<em>RoPE</em>; <a href="https://arxiv.org/abs/2104.09864">Su et al. 2021</a>) encodes the absolution position with a <a href="https://en.wikipedia.org/wiki/Rotation_matrix">rotation matrix</a> and multiplies key and value matrices of every attention layer with it to inject relative positional information at every layer.<br/></p>
<p>When encoding relative positional information into the inner product of the $i$-th key and the $j$-th query, we would like to formulate the function in a way that the inner product is only about the relative position $i-j$. Rotary Position Embedding (RoPE) makes use of the rotation operation in Euclidean space and frames the relative position embedding as simply rotating feature matrix by an angle proportional to its position index.<br/></p>
<p>Given a vector $\mathbf{z}$, if we want to rotate it counterclockwise by $\theta$, we can multiply it by a rotation matrix to get $R\mathbf{z}$ where the rotation matrix $R$ is defined as:</p>
<div>
$$
R = \begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}
$$
</div>
<p>When generalizing to higher dimensional space, RoPE divide the $d$-dimensional space into $d/2$ subspaces and constructs a rotation matrix $R$ of size $d \times d$ for token at position $i$:</p>
<div>
$$
R^d_{\Theta, i} = \begin{bmatrix}
\cos i\theta_1 & -\sin i\theta_1 & 0 & 0 & \dots & 0 & 0 \\
\sin i\theta_1 & \cos i\theta_1 & 0 & 0 & \dots & 0 & 0 \\
0 & 0 & \cos i\theta_2 & -\sin i\theta_2 & \dots & 0 & 0 \\
0 & 0 & \sin i\theta_2 & \cos i\theta_2 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \dots & \cos i\theta_{d/2} & -\sin i\theta_{d/2} \\
0 & 0 & 0 & 0 & \dots & \sin i\theta_{d/2} & \cos i\theta_{d/2} \\
\end{bmatrix}
$$
</div>
<p>where in the paper we have $\Theta = {\theta_i = 10000^{-2(iâˆ’1)/d}, i \in [1, 2, &hellip;, d/2]}$. Note that this is essentially equivalent to sinusoidal positional encoding but formulated as a rotation matrix.</p>
<p>Then both key and query matrices incorporates the positional information by multiplying with this rotation matrix:</p>
<div>
$$
\begin{aligned}
& \mathbf{q}_i^\top \mathbf{k}_j = (R^d_{\Theta, i} \mathbf{W}^q\mathbf{x}_i)^\top (R^d_{\Theta, j} \mathbf{W}^k\mathbf{x}_j) = \mathbf{x}_i^\top\mathbf{W}^q R^d_{\Theta, j-i}\mathbf{W}^k\mathbf{x}_j \\
& \text{ where } R^d_{\Theta, j-i} = (R^d_{\Theta, i})^\top R^d_{\Theta, j}
\end{aligned}
$$
</div>
<figure>
	<img src="RoPE.png" style="width: 100%;"  />
	<figcaption>Visual illustration of how rotary position embedding is implemented.(Image source: <a href="https://arxiv.org/abs/2104.09864" target="_blank">Su et al., 2021</a>) Note: I used $i$ instead of $m$ to represent the position index compared to the original figure in the paper.</figcaption>
</figure>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] Ashish Vaswani, et al. <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">&ldquo;Attention is all you need.&rdquo;</a> NIPS 2017.</p>
<p>[2] Lilian Weng. <a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">&ldquo;The Transformer Family&rdquo;</a> Lil'Log, 2020.</p>
<p>[3] Christopher M. Bishop. <a href="https://www.bishopbook.com/">&ldquo;Deep Learning: Foundations and Concepts&rdquo;</a> Springer, 2026.</p>


  </div>
</div></article></main><footer class="footer"><span>&copy; 2026 <a href="/">Technical Basics</a></span></footer></body></html>
